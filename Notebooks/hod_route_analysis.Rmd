---
title: "Traffic patterns from UBER Movement Dataset"
output: html_notebook
---

Datasets:

|     Key        |        Value                                                     |
|-------------|-------------------------------------------------------------|
| Time Period | Q1 2018                                                     |
| Dataset     | Aggregated travel times between every zone pair in the city |
| City        | Delhi                                                       |

```{r}
library(tidyverse)
new_delhi_wards <- read.csv("Data/uber_delhi_ward_ids.csv", stringsAsFactors = FALSE)
inter_ward_times <- data.table::fread("Data/new_delhi-wards-2018-1-All-HourlyAggregate.csv", stringsAsFactors = F)
```

Inter ward distance was calculated through QGIS using the the wards geojson file downloaded from the UBER Movement website for DELHI. Process was:

- To first find the center of each polygon
- To calculate a distance matrix from each polygon center to other for all ploygons

```{r}
inter_ward_distance <- data.table::fread("Data/spatial/distance_matrix.csv", stringsAsFactors = F, data.table = FALSE)
```

Merging Distance and Time components

```{r}
inter_ward_matrix <-
  dplyr::left_join(
    inter_ward_times,
    inter_ward_distance,
    by = c('sourceid' = 'InputID', 'dstid' = 'TargetID')
  )
```

Converting distance to kilometer
```{r}
inter_ward_matrix$distance_km <- inter_ward_matrix$distance_metre/1000
```

The objective of this analysis is to focus on the congestion of vehicular activity on
city routes. Since the current dataset from UBER gives us just the travel times, it is importatnt to normlaise it by the distance metric, thus focusing on time taken on covering every km of a route (assumption being that every km takes the equal amount of time, certainly not the case, but this should be good for an EDA) and treating it as a proxy of business of that route. 

```{r}
inter_ward_matrix$time_per_km <- inter_ward_matrix$mean_travel_time/inter_ward_matrix$distance_km
```

Let's now look at the density plots for time per km at every hour of the day - This will help us see the range of times and check if the flow rate of city traffic is constant at every hour or varies across routes

Excluding the rides done in the same Movement zone, as the distance is not calculated for such cases. 
```{r}
library(ggplot2)

ggplot(inter_ward_matrix[!is.na(inter_ward_matrix$distance_degree),], aes(x = time_per_km)) +
  geom_density(alpha=.2, fill="#FF6666") +
  facet_wrap(~ hod, nrow=6) + xlab("Time per km (seconds)")
```

Results:

- There are high peaks from 0-6 hours as the congestion seems to be less at these points, so most rides are able to maintain a good flow rate, 75% of these rides cover a distance of a km in less than *150* seconds
- The peaks turns a bit flat after 7 AM and the width increases, as the congestion increases, so is the variability with time.

Let's look at the 90th percentile of *time taken per km* at every hour
```{r}
quantile_vec <- c()
for(i in 1:24){
 quantile_vec <- c(quantile_vec,quantile(inter_ward_matrix$time_per_km[inter_ward_matrix$hod==(i-1) & !is.na(inter_ward_matrix$distance_degree)], 0.9))
}
quantile_df <- data.frame('hod' = 0:23, quantile_vec)
ggplot(data = quantile_df, aes(x = hod,y = quantile_vec)) + geom_bar(stat = 'identity') +
  ylab("Time per km (95th percentile)") + xlab('Hour of the day')
```

Observations:

Being rush office hours, 

- First peak is around 11-13, where TPK (Time per km) is close to 300s
- Second peak is around 17-20 where the TPK goes almost till 350s

Till now we were only working with hod(Hour of the day), now lets take the route into consideration as well

Busiest routes throughout the day (Top 10): 

```{r}
x <- inter_ward_matrix %>% filter(!is.na(time_per_km))%>% group_by(sourceid, dstid) %>%
  summarise(mean_tpk = mean(time_per_km))
x <- dplyr::left_join(x, inter_ward_distance, by = c('sourceid' = 'InputID', 'dstid' = 'TargetID'))
x <- x[order(-x$mean_tpk), ]
x[1:10,c(1,2,3,5)]
```

> Shorter routes will tend to have a greater TPK, as for larger routes this metric gets time to decrease because of more occurrences of light congestion segments as compared to shorter routes. 

Let's look at the same table again but now between routes which are atleast greater than a km. 
```{r}
x <- x[x$distance_metre >= 1000, ]
x <- x[order(-x$mean_tpk), ]
x[1:10,c(1,2,3,5)]
```

Observations: 

These are actually pretty congested areas - 

- Though shorted distances, the average time is ~15 mins
- The time taken between these wards was verified on Google Maps and the results were pretty close
- These areas lies in region with high population and high commercial activities, not so good roads and a heavy traffic at pretty much any time of the day

The satellite view - one of the busiest route (181 - 178) looks like this
![busy_routes](../images/busy_routes.png)


The UBER Movement web UI is amazing, its good from a single user perspective, but if a city planner wants to have compare mutiple routes at once, then it can be a bit difficult. You are always tied to a source and a destination for certain analyses, and though the avergae time for every ward from a source can be identified by a map view, the same thing cannot be done for a destination. 

Let's fix our destination at the Delhi Airport Ward (There is no dedicated ward for this, so taking the closest one - Ward No 5 as a proxy) and look at average times (TPK) from every other ward
```{r}
dst5 <- inter_ward_matrix[inter_ward_matrix$dstid==5,]
dst5 %>% group_by(sourceid) %>% summarise(mean_tpt = mean(time_per_km)) %>% top_n(5, mean_tpt) %>%   data.frame()
```

The results are same as above, short routes having a higher TPK, but the last two sources are interesting. 

- Both of them are over 10 Km's. 
- Source 288 lies in between Source 255 and Airport
- The route starts from the eastern parts of Delhi and ends at the Southern part
